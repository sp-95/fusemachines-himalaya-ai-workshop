{"cells":[{"cell_type":"markdown","source":["## [CRISP DM Methodology](https://drive.google.com/file/d/1kNAxC9_HPtyQCSDLrcdK4fZw2EEGQpek/view?usp=sharing)\n","1. Business Understanding  \n"," a) Focus on understanding the objectives and requirements of the project\n","2. Data Understanding  \n","  a) Collect initial data  \n","  b) Describe data  \n","  c) Explore data  \n","  d) Verify data quality  \n","3. Data Preparation  \n","  a) Select data  \n","  b) Clean data  \n","  c) Construct data  \n","  d) Integrate data  \n","  e) Format data  \n","4. Modeling\n","5. Evaluation\n","6. Deployment (not covered)\n","\n","--- \n","\n","<img src = \"https://i.pinimg.com/originals/02/81/f9/0281f9d2d8b8c9f2801843a1f7445977.png\" />\n","\n"],"metadata":{"id":"HUNDXjA6FycS"}},{"cell_type":"markdown","metadata":{"id":"p-s-a8nPoRHY"},"source":["**<h1>Business Understanding: Problem Statement</h1>**\n","\n","<li><b>According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally.</b></li>\n","\n","<li><b>Stroke is responsible for approximately 11% of total deaths in the world</b></li>\n","\n","<li><b>The dataset is provided to predict whether a patient is likely to get stroke or not on the basis of given input parameters</b></li>\n","\n","<li><b>Each row in the data provides relavant information about the patient.</b></li>\n","\n","<li><b>After going through the context of datasets, we can say that it is a binary classification problem</b></li>\n","\n","<li><b>We have to make prediction on the target variable STROKE</b></li>\n","\n","<li><b>Finally, we have to build a model to get the best prediction on the stroke variable</b></li>"]},{"cell_type":"markdown","metadata":{"id":"Uv6lFu_0n4Z_"},"source":["\n","___________________________________________________________\n","\n","**<h4>Let's get started</h4>**\n","___________________________________________________________"]},{"cell_type":"markdown","source":["# Data Understanding"],"metadata":{"id":"kt9Dsw4pNxrL"}},{"cell_type":"markdown","metadata":{"id":"qebqtOrln4Z_"},"source":["## Data Ingestion"]},{"cell_type":"markdown","metadata":{"id":"D5AfDboxn4aA"},"source":["**[Term] Data Ingestion**\n","\n","Data ingestion is the process of obtaining and importing data for immediate use or storage."]},{"cell_type":"markdown","metadata":{"id":"Xlszj9Gkn4aA"},"source":["For this session we'll \"ingest\" the data from google drive and save it as a csv file using the following code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zlG9S61eRpUC"},"outputs":[],"source":["import gdown\n","\n","dataset_url = \"https://drive.google.com/u/1/uc?id=1Wt1eQkZz76gqRL9h8R6FOEPH3eLsbLsA&export=download\"\n","filename = \"healthcare-dataset-stroke-data.csv\"\n","gdown.download(dataset_url, filename)"]},{"cell_type":"markdown","metadata":{"id":"zTYxsoCRn4aD"},"source":["**[Library] NumPy**\n","\n","[NumPy](https://numpy.org/) makes it easier to manupulate large, multi-dimensional arrays and matrices, and provides high-level mathematical functions to operate on them."]},{"cell_type":"markdown","metadata":{"id":"_6Bfcp1Tn4aD"},"source":["**[Library] Pandas**\n","\n","[Pandas](https://pandas.pydata.org/) is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool. It stands for \"Python Data Analysis Library\"."]},{"cell_type":"markdown","metadata":{"id":"uu2RRfxGn4aE"},"source":["Let's import numpy and pandas as we'll be using them a lot for the upcoming steps."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lkl-M_wfn4aE"},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"1j-U2guGn4aF"},"source":["**[Term] Dataframe**\n","\n","A pandas Dataframe is a two-dimensional data structure i.e. data is aligned in a tabular fashion in rows and columns."]},{"cell_type":"markdown","metadata":{"id":"cWGRHXTWVLb5"},"source":["We will use the method `read_csv()` of a pandas dataframe to load the data from the saved csv file. We will also use the method `head()` to display the first five rows of the dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cCdvEXt4Utpc"},"outputs":[],"source":["df = pd.read_csv(filename)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"U8_MhUvWn4aG"},"source":["We can call the `info()` method of the dataframe to print it's information including the index (#), datatype (dtype) and columns, non-null values (will be discussed later) and memory usage."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PUigiQeDZpQt"},"outputs":[],"source":["## Use info() method to print dataframe information\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"hMhW5LSS4pjp"},"source":["### **Insights at a Glance**\n","\n","<li>What did you observe after taking a peek at the information on the dataset? List your insights here</li>\n"]},{"cell_type":"markdown","metadata":{"id":"6fbUr83GaaAB"},"source":["## Exploratory Data Analysis and Data Cleaning"]},{"cell_type":"markdown","metadata":{"id":"HXQB9iK8n4aH"},"source":["**[Term] Exploratory Data Analysis (EDA)**\n","\n","EDA is an approach of analyzing data sets to summarize their main characteristics"]},{"cell_type":"markdown","metadata":{"id":"bZyNzAKcn4aI"},"source":["**[Term] Data Cleaning**\n","\n","Data cleaning is the process of detecting and correcting corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data."]},{"cell_type":"markdown","metadata":{"id":"bhVqzHqCn4aI"},"source":["### Missing Data"]},{"cell_type":"markdown","metadata":{"id":"UrKR5jMN_rzi"},"source":["In programming we refer to missing values as a *null* value.\n","\n","We can use the following functions to identify missing values:\n","  1. `isnull()`\n","  2. `notnull()`\n","\n","The output is a boolean value indicating whether the value that is passed into the argument is in fact missing data.\n","\n","**Note:**\n","  - `isnull()` is an alias for `isna()`\n","  - `notnull()` is an alias for `notna()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HheSbC1sC1wn"},"outputs":[],"source":["## Display null values in the dataframe\n","## Hint: : use isnull() method\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"xHVnmmj-Fbqz"},"source":["In the output above, `True` denotes a *null* value and `False` denotes a *non-null* value."]},{"cell_type":"markdown","metadata":{"id":"K0-l0oZ1n4aJ"},"source":["**Q:** What will be the output when you add `False` and `False`?\n","```python\n","False + False\n","```"]},{"cell_type":"markdown","metadata":{"id":"BTFtXb-Cn4aJ"},"source":["We can take a column-wise sum using the method `sum()` to get the count of the number of missing values in each column.  \n","- axis=0, row wise operation\n","- axis=1, column wise operation\n","- For pictorial representation [click_here](https://www.google.com/search?q=pandas+axis+0+and+1&sxsrf=ALiCzsbQnl1dNx8f56SReKo5AHtWDPQmqg:1658113179775&source=lnms&tbm=isch&sa=X&ved=2ahUKEwidu8_4uIH5AhXs9nMBHbfEBa0Q_AUoAXoECAEQAw&biw=1536&bih=552&dpr=1.25#imgrc=gPcqaRZ_oIIdYM)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qf_y5Brpn4aJ"},"outputs":[],"source":["## Get total missing values in each column (features)\n","## Hint: use sum() method\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"DJQOhIuun4aK"},"source":["Do you see any missing data?"]},{"cell_type":"markdown","metadata":{"id":"WJRKRJ0sn4aK"},"source":["There are two ways to access a single column of a DataFrame\n","  * `df.column_name`\n","  * `df[\"column_name\"]`"]},{"cell_type":"markdown","metadata":{"id":"dBWR11Iln4aK"},"source":["**Q:** How can we display the first five rows of the column **bmi**?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2dBKrLOQn4aK"},"outputs":[],"source":["## Display first five rows of the column bmi\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"dYsKIvuFn4aL"},"source":["We can see **NaN** values in the **bmi** column; these are the missing values which may hinder our further analysis."]},{"cell_type":"markdown","metadata":{"id":"aM4TAmWnn4aL"},"source":["#### Replacing missing values\n","\n","- **NaN** (`np.nan`) is python's default marker for missing value and it stands for \"Not a Number\". \n","- We need to replace various missing values (such as empty string, ?, null, etc) with python's default missing value marker i.e. **NaN**, so that it will be easier to handle the missing values of a dataset later. Let's see two different approach to convert missing values to python's default missing value marker.\n","\n","  - Approach 1: Replace the missing values **while** reading the data\n","    - Make a list of different missing values i.e. `missing_values = [\"?\", \"\", \"n/a\", \"--\"]`\n","    - Pass this list in the `na_values` parameter while reading the data using pandas\n","      ```python\n","      df = pd.read_csv(\n","        \"healthcare-dataset-stroke-data.csv\",\n","        na_values=missing_values,\n","      )\n","      ```\n","\n","  - Approach 2: Replace the missing values **after** reading the data\n","    - Pandas provides a `replace()` method which can be used to replace the missing values\n","      ```python\n","      df = df.replace(\"?\", np.nan)\n","      ```\n","  - If you want to avoid assigning the new DataFrame to the same variable you can set it \"in place\" using the `inplace` parameter\n","    ```python\n","    df.replace(\"?\", np.nan, inplace=True)\n","    ```\n","\n","In our dataset, missing values are already represented as python's default missing value marker. So we can skip the steps above for now."]},{"cell_type":"markdown","metadata":{"id":"cMj5knzCn4aL"},"source":["#### Missing value visualization"]},{"cell_type":"markdown","metadata":{"id":"RdzOXJqkn4aL"},"source":["**[Library] Matplotlib**\n","\n","Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations."]},{"cell_type":"markdown","metadata":{"id":"WvbOLnwOn4aL"},"source":["**[Library] Seaborn**\n","\n","Seaborn is a data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics."]},{"cell_type":"markdown","metadata":{"id":"u_JpwP6Kn4aL"},"source":["Let's import these libraries first to get started with visualization\n","\n","**[Term] Heatmap**  \n","A heatmap is a graphical representation of data where each value of a matrix is represented as a color."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sfUw9sHRn4aM"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"XaEqMH9Kn4aM"},"source":["Let us now try to visualize where our missing values lie within the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFpYGsKSaWsN"},"outputs":[],"source":["plt.figure(figsize=(10, 10))\n","plt.title(\"Visualizing Missing Values\")\n","sns.heatmap(\n","    df.isna().transpose(),\n","    cmap=\"YlGnBu\",  # Set a yellow -> green -> blue color gradient\n","    cbar_kws={\"label\": \"Missing Data\"}\n",")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"X9h0DMrhhGvF"},"source":["<li>List your observations here</li>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bkwSQQVVn4aN"},"source":["### Univariate Analysis"]},{"cell_type":"markdown","metadata":{"id":"l1mARXujn4aO"},"source":["**[Term] Univariate Analysis**\n","\n","Univariate analysis is the technique of comparing and analyzing the dependency of a single predictor and a response variable. The prefix \"uni\" means one, emphasizing the fact that the analysis only accounts for one variable's effect on a dependent variable."]},{"cell_type":"markdown","metadata":{"id":"zPinwGWMn4aO"},"source":["#### Categorical Variables"]},{"cell_type":"markdown","metadata":{"id":"qATHP6Rsn4aO"},"source":["**[Term] Categorical Variable**\n","\n","In statistics, a categorical variable is a variable that can take on one of a limited, and usually fixed, number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property."]},{"cell_type":"markdown","metadata":{"id":"Bbc_ege-n4aO"},"source":["For a categorical variable we need to first count the number of data points in each category to analyze the data distribution."]},{"cell_type":"markdown","metadata":{"id":"2c3kDc8Tn4aO"},"source":["#### gender"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XXReMeIqn4aO"},"outputs":[],"source":["## Count each category in the column named \"gender\"\n","## Hint: use value_counts() \n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"XHJyVIFjn4aP"},"source":["**Q:** What kind of plot would be the best to visualize value counts?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cgxZkxItn4aP"},"outputs":[],"source":["## Visualize the `value_counts()` using the plot you think would be the best\n","## Hint: pandas provides in-built plotting functions for DataFrames\n","## * ‘bar’ or ‘barh’ for bar plots\n","## * ‘hist’ for histogram\n","## * ‘box’ for boxplot\n","## * ‘kde’ or ‘density’ for density plots\n","## * ‘area’ for area plots\n","## * ‘scatter’ for scatter plots\n","## * ‘hexbin’ for hexagonal bin plots\n","## * ‘pie’ for pie plots\n","## For example `df.plot.pie()` will display a pie chart\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"VvsYuaWon4aP"},"source":["Let's stylize it and put it in a function for ease of use"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1--0cR_fn4aP"},"outputs":[],"source":["def univariate_barplot(data, xlabel, ylabel=\"Frequency\", xlabel_rotation=0):\n","    # Set font size\n","    sns.set(font_scale=1.4)\n","\n","    # Get value counts\n","    value_counts = data[xlabel].value_counts()\n","    ax = value_counts.plot.bar(\n","        # Set a categorical color palette\n","        color=sns.color_palette(\"Set2\"),\n","        # Set a figure size\n","        figsize=(10, 10),\n","        # Rotate the x-axis labels for proper orientation\n","        rot=xlabel_rotation,\n","    )\n","\n","    total = value_counts.sum()\n","    # Add percentage label for each plot\n","    for plot in ax.patches:\n","        height = plot.get_height()\n","        ax.annotate(\n","            # Percentage label text\n","            f\"{height / total * 100:.1f}%\",\n","            xy=(\n","                # Percentage label x co-ordinate\n","                plot.get_x() + plot.get_width() / 2.,\n","                # Percentage label y co-ordinate\n","                height,\n","            ),\n","            # Percentage label horizontal align\n","            ha=\"center\",\n","\n","        )\n","\n","    ax.set_title(f\"Distribution of {xlabel} in the dataset\", y=1.02)\n","    ax.set_xlabel(xlabel, labelpad=14)\n","    ax.set_ylabel(ylabel, labelpad=14)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OW3gX__Wn4aQ"},"outputs":[],"source":["## Display plot for \"gender\" \n","## Hint: use univariate_barplot() function \n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"ZaAuD6_gis65"},"source":["<li>What are your observations on the \"gender\" column?</li>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4KHGIO5An4aQ"},"outputs":[],"source":["## Filter out \"Other\" from \"gender\" and reset dataframe index\n","## Hint: use reset_index() method to reset the index\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFuXP3-in4aQ"},"outputs":[],"source":["## Display plot for \"gender\" again to visualize the difference\n","## Hint: use univariate_barplot() function \n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"pGLAZ8BtoxVX"},"source":["#### ever_married"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nb4y2tSun4aR"},"outputs":[],"source":["## Display plot for \"ever_married\"\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"uJswDt8YpZgH"},"source":["<li>List your observations for marriage status</li>"]},{"cell_type":"markdown","metadata":{"id":"vwlViBGLruvf"},"source":["#### Residence_type"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eifoRkPmpwXC"},"outputs":[],"source":["## Display plot for residence_type\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"YqASQoyR7qcT"},"source":["<li>Observations on Residence type here</li>\n"]},{"cell_type":"markdown","metadata":{"id":"GXNHfDyVIYHn"},"source":["**<h4>work_type</h4>**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kkc1bgNdIYUM"},"outputs":[],"source":["## Display plot for \"work_type\"\n","## Hint: use univariate_barplot() function, pass argument xlabel_rotation=45\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"4peAxoDXM1WU"},"source":["<li>Work type observations</li>"]},{"cell_type":"markdown","metadata":{"id":"JBhbMCHLZuLn"},"source":["#### smoking_status"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ycHPBXo67I4u"},"outputs":[],"source":["## Display plot for \"smoking\"\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"Qtcj1AlJpkkR"},"source":["<li>Smoking status observations</li>\n"]},{"cell_type":"markdown","metadata":{"id":"z2AS_3Qyn4aT"},"source":["#### Numerical Variables"]},{"cell_type":"markdown","metadata":{"id":"9a4lWdjvn4aT"},"source":["**[Term] Numerical Variable**\n","\n","In statistics, A numeric variable (also called quantitative variable) is a quantifiable characteristic whose values are numbers. Numeric variables may be either continuous or discrete."]},{"cell_type":"markdown","metadata":{"id":"UxzV3cwSS3s8"},"source":["#### hypertension"]},{"cell_type":"markdown","metadata":{"id":"ioSWyAM6n4aT"},"source":["We use the `describe()` method of a pandas dataframe to analyze a numeric variable's data distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UclrNS_Tn4aT"},"outputs":[],"source":["df.hypertension.describe()"]},{"cell_type":"markdown","metadata":{"id":"FlMYUoJ4n4aT"},"source":["As you can see, almost all the values of **hypertension** seems to be either a \"0\" or a \"1\". So it is better to treat it as a categorical value rather than a numerical value. Let's perform categorical analysis on **hypertension**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"50vHzfYXRVaf"},"outputs":[],"source":["## Display bar plot for \"hypertension\" field\n","## Hint: use previous function univariate_barplot()\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"HqIbsy3YW6J-"},"source":["<li>List your observations here</li>"]},{"cell_type":"markdown","metadata":{"id":"8WZZv8NOYGCg"},"source":["#### heart_disease"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BN3pGbs6n4aU"},"outputs":[],"source":["## Use describe() method for heart_disease\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"BiIKP0YRn4aU"},"source":["What did you observe for the **heart_disease** column? How does it compare with the **hypertension** column?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"urJQkTgNRVd2"},"outputs":[],"source":["## Display bar plot for \"heart_disease\" field\n","## Hint: use previous function univariate_barplot()\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"XntVfFBtYXSL"},"source":["<li>List your observations here</li>"]},{"cell_type":"markdown","source":["Q: How many categorical features are there in our dataset?"],"metadata":{"id":"VruEmO1RYMhD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DyinMyrtNkMh"},"outputs":[],"source":["## List all the categorical features that you observed in the data\n","\n","categorical_features = [\n","### START CODE HERE ###\n","  \"\",\n","  \"\",\n","### END CODE HERE ###\n","]"]},{"cell_type":"markdown","metadata":{"id":"5eaAFjmhn4aV"},"source":["Q: How many numerical features are there in our dataset?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVn5L5Omn4aW"},"outputs":[],"source":["## List all the categorical features that you observed in the data\n","\n","numerical_features = [\n","### START CODE HERE ###\n","  \"\",\n","  \"\",\n","### END CODE HERE ###\n","]"]},{"cell_type":"markdown","metadata":{"id":"my40ofJZn4aV"},"source":["#### Bonus Content (Optional)\n","\n","You can create an interactive plot for the ease of traversing through your plots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tNiOmCvQn4aV"},"outputs":[],"source":["from ipywidgets import interact"]},{"cell_type":"markdown","metadata":{"id":"mxNOEQT2n4aV"},"source":["**[Term] Decorators**\n","\n","A decorator is a design pattern in Python that allows a user to add new functionality to an existing object without modifying its structure. Decorators are usually called before the definition of a function you want to decorate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9DnsSQGsn4aV"},"outputs":[],"source":["@interact(predictor=categorical_features)\n","def interactive_univariate_barplot(predictor):\n","    univariate_barplot(df, predictor)"]},{"cell_type":"markdown","source":["As mentioned before, we can use the `describe()` method of a pandas dataframe to analyze a numeric variable's data distribution"],"metadata":{"id":"zgUiZnNIZLG1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9yUCg7OmRPxJ"},"outputs":[],"source":["## Use describe() method to get descriptive stats of numerical features\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"65_dgrXrRpqD"},"source":["<li>List your observations here</li>"]},{"cell_type":"markdown","metadata":{"id":"Xtjj82tCn4aW"},"source":["Other than the data description the best way to visualize a numerical variable is to look at its distribution plot"]},{"cell_type":"markdown","metadata":{"id":"h3Bi6f_pn4aW"},"source":["**[Term] Kernel Density Estimation (KDE)**\n","\n","KDE is a way to estimate the probability density function of a continuous random variable."]},{"cell_type":"markdown","metadata":{"id":"mT3k8XwOn4aW"},"source":["#### age"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMHPQdRJn4aW"},"outputs":[],"source":["sns.displot(data=df.age, kde=True, height=10)"]},{"cell_type":"markdown","metadata":{"id":"w-3t3dyBn4aX"},"source":["#### bmi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"de2gjrDsn4aX"},"outputs":[],"source":["## Display distribution plot for \"bmi\" field\n","## Hint: use sns.distplot()\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"Jv5dK30Cn4aX"},"source":["#### avg_glucose_level"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJxRN-POn4aX"},"outputs":[],"source":["## Display distribution plot for \"avg_glucose_level\" field\n","## Hint: use sns.distplot()\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"VzseYdDOn4aX"},"source":["**[Term] Data Skewness**\n","\n","In probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. Direction of skew signifies where the \"tail\" of the distribution lies at."]},{"cell_type":"markdown","metadata":{"id":"8n4lyqzvn4aX"},"source":["Look at the mean, median and the plots and list your observations below:\n","\n","<li>Your observations here</li>"]},{"cell_type":"markdown","metadata":{"id":"eo9ugZ1on4aY"},"source":["Use the `skew()` method to verify your observation. A negative value signifies a left skew and a positive value signifies a right skew."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mybohBJqRPzZ"},"outputs":[],"source":["df[numerical_features].skew()"]},{"cell_type":"markdown","metadata":{"id":"k_fEaQfhn4aY"},"source":["### Target Variable Analysis: stroke"]},{"cell_type":"markdown","metadata":{"id":"Frxzze-Kn4aY"},"source":["One of the first steps of exploratory data analysis should always be to look at what the values of the label looks like."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fAK-Vfpxn4aY"},"outputs":[],"source":["target_variable = \"stroke\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C-wKKL69n4aY"},"outputs":[],"source":["## Display barplot for our target variable\n","## Hint: use univariate_barplot() function\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"ACvEAIsRn4aZ"},"source":["<li>Your observations here</li>"]},{"cell_type":"markdown","metadata":{"id":"PEsMDFRfn4aZ"},"source":["**[Term] Imbalanced Dataset**\n","\n","Imbalanced data refers to those types of datasets where the target class has an uneven distribution of observations, i.e one class label has a very high number of observations and the other has a very low number of observations."]},{"cell_type":"markdown","metadata":{"id":"GGh-Gngjn4aZ"},"source":["We will revisit the problem of an imbalanced dataset later"]},{"cell_type":"markdown","metadata":{"id":"jb5AXpNPn4aZ"},"source":["### Bivariate Analysis"]},{"cell_type":"markdown","metadata":{"id":"-P05tOy2n4aZ"},"source":["**[Term] Bivariate Analysis**\n","\n","Bivariate analysis is one of the simplest forms of quantitative analysis. It involves the analysis of two variables, for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association."]},{"cell_type":"markdown","metadata":{"id":"f6LcCDkpn4aZ"},"source":["*pandas* provides a `groupby()` function which allows us to group large amounts of data and compute operations on these groups."]},{"cell_type":"markdown","metadata":{"id":"1JbgXd2un4aa"},"source":["Let us first retrace our step back to *work_type* and *smoking_status* and re-analyze them using bivariate analysis. Since they both seem to be associated with age let's compare them with respect to age."]},{"cell_type":"markdown","metadata":{"id":"TYmEfIL-n4aa"},"source":["#### work_type vs age"]},{"cell_type":"markdown","source":["pandas provides an `agg()` method on groupby objects to aggregate using one or more operations over the specified axis."],"metadata":{"id":"B2SrG1zMaFO7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ujKtyOX_n4aa"},"outputs":[],"source":["df.groupby(\"work_type\").agg({\"age\": \"describe\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWwjWxttn4aa"},"outputs":[],"source":["ax = df.plot.scatter(x=\"work_type\", y=\"age\", rot=45, figsize=(10, 10))\n","ax.set_title(\"work_type vs age\", y=1.02)\n","ax.axhline(y=13, c=\"r\", linestyle=\"dashed\")\n","ax.axhline(y=16, c=\"g\", linestyle=\"dashed\")"]},{"cell_type":"markdown","metadata":{"id":"mlrK0zqNpgXb"},"source":["**<h5>Let's make some assumptions on the data</h5>**\n","\n","<li><b>Bivariate analysis is done to understand appropriate age group for certain work types</b></li>\n","\n","<li><b>Since, children are not allowed to work so we can change these values to Never_worked category</b></li>\n","\n","<li><b>There seems to be some outliers in Private and Self-employed work types</b></li>\n","\n","<li><b>Let's assume that people below 13 years of age can not be employed for a private job as it is a minimum criteria for many countries</b></li>\n","\n","<li><b>These days lots of people can be self-employed from YouTube. However, the minimum age limit for youtuber is 13 years. Considering this situation, let's assume that the minimum age limit for being self-employed to be 13 years</b></li>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53MODe2Z2ELh"},"outputs":[],"source":["df.loc[df[\"work_type\"] == \"children\", \"work_type\"] = \"Never_worked\"\n","df.loc[(df[\"work_type\"] == \"Private\") & (df[\"age\"] < 13), \"age\"] = 13\n","df.loc[(df[\"work_type\"] == \"Self-employed\") & (df[\"age\"] < 13), \"age\"] = 13"]},{"cell_type":"markdown","metadata":{"id":"RKhm25wMn4ab"},"source":["Let us now look at the updated plots of *work_type*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9pKmvC2mn4ab"},"outputs":[],"source":["univariate_barplot(df, \"work_type\", xlabel_rotation=45)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQqJ0Fv3n4ab"},"outputs":[],"source":["ax = df.plot.scatter(x=\"work_type\", y=\"age\", rot=45, figsize=(10, 10))\n","ax.set_title(\"work_type vs age\", y=1.02)"]},{"cell_type":"markdown","metadata":{"id":"GZQ7-JIhn4ac"},"source":["We have now reduced the work_type to just 4 categories and handled some of the outliers in the simplest way possible."]},{"cell_type":"markdown","metadata":{"id":"ibJaBrTOn4ac"},"source":["#### smoking_status vs age"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8MNgReEMn4ac"},"outputs":[],"source":["## Group by 'smoking_status' and aggregate using the `describe` method\n","## Hint: use groupby() and agg()\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Ar6Lqyan4ad"},"outputs":[],"source":["ax = df.plot.scatter(x=\"smoking_status\", y=\"age\", rot=45, figsize=(10, 10))\n","ax.set_title(\"smoking_status vs age\", y=1.02)\n","ax.axhline(y=10, c=\"r\", linestyle=\"dashed\")"]},{"cell_type":"markdown","metadata":{"id":"pJ_4ihWqpatA"},"source":["**<h5>Let's make some more assumptions on the data</h5>**\n","\n","<li><b>We can clearly see that people below 10 years are not found smoking in this dataset</b></li>\n","\n","<li><b>Let's make an assumption that person below 10 years can not smoke</b></li>\n","\n","<li><b>But there are some people whose smoking status is unknown even if they are below that age limit</b></li>\n","\n","<li><b>People whose smoking status is unknown and below 10 years age limit can thus be changed to never smoked</b></li>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ktQcWlYI7I_P"},"outputs":[],"source":["df.loc[(df[\"smoking_status\"] == \"Unknown\") & (df[\"age\"] < 10), \"smoking_status\"] = \"never smoked\""]},{"cell_type":"markdown","metadata":{"id":"izxtvPpmn4ae"},"source":["Let us now look at the updated plot of *smoking_status*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JYzp5_Abn4ae"},"outputs":[],"source":["## Display the scatter plot of smoking status vs age the same as was done before\n","## assumptions were made on the data\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"2fVLj7eWn4af"},"source":["It's quite hard to make an assumption on the remaining *Unknown* *smoking_staus*. We can convert it to pandas missing value marker (NaN) since they're essentially missing values but let's leave them as it is for now."]},{"cell_type":"markdown","metadata":{"id":"DlmrhveNn4af"},"source":["**Q:** How can you replace the *Unknown* with a pandas missing value marker (`np.nan`)?"]},{"cell_type":"markdown","metadata":{"id":"Ac8P0TLVn4af"},"source":["#### gender vs stroke"]},{"cell_type":"markdown","metadata":{"id":"qlppiI_Fn4af"},"source":["Let us now look at the distribution of our target variable within a categorical variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r0As_d8Hn4ag"},"outputs":[],"source":["df.groupby([\"gender\", \"stroke\"]).size()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xC7pMyUZn4ag"},"outputs":[],"source":["df.groupby([\"gender\", \"stroke\"]).size().unstack()"]},{"cell_type":"markdown","metadata":{"id":"pTOHJC91n4ag"},"source":["Let's make a visualization similar to our univariate analysis but with the added information of the target variable distribution."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSseHEArn4ag"},"outputs":[],"source":["def bivariate_barplot(data, xlabel, ylabel=\"Frequency\", xlabel_rotation=0):\n","    # Set font size\n","    sns.set(font_scale=1.4)\n","    \n","    ax = df.groupby([xlabel, \"stroke\"]).size().unstack().plot.bar(\n","        stacked=True,\n","        # Set a categorical color palette\n","        color=sns.color_palette(\"Set2\"),\n","        # Set a figure size\n","        figsize=(10, 10),\n","        # Rotate the x-axis labels for proper orientation\n","        rot=xlabel_rotation,\n","    )\n","\n","    ax.set_title(f\"Distribution of {xlabel} with respect to the stroke\", y=1.02)\n","    ax.set_xlabel(xlabel, labelpad=14)\n","    ax.set_ylabel(ylabel, labelpad=14)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"21emIVAyn4ah"},"outputs":[],"source":["bivariate_barplot(df, \"gender\")"]},{"cell_type":"markdown","metadata":{"id":"c-o3q8zln4ah"},"source":["Let's put this into a different perspective by calculating the percentage probability of each category having a stroke.\n","\n","* We will first need the data of stroke victims for each category. Since the stroke values are just 0s and 1s, we can easily calculate it if we `groupby()` the category and then add the stroke values.\n","* To calculate the percent probability we'll need the sample size as well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fib_OVzln4ah"},"outputs":[],"source":["feature_impact = df.groupby(\"gender\")[\"stroke\"].agg([\"sum\", \"size\"])\n","feature_impact"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FBkhqvcyn4ai"},"outputs":[],"source":["feature_impact_probability = feature_impact[\"sum\"] / feature_impact[\"size\"] * 100\n","feature_impact_probability"]},{"cell_type":"markdown","metadata":{"id":"4q3muy6kwx4g"},"source":["<li>List your observations here</li>"]},{"cell_type":"markdown","metadata":{"id":"nsBQ2aLvn4ai"},"source":["Let's wrap this up in a function to reuse it in the future"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwxjiF0on4ai"},"outputs":[],"source":["def calculate_feature_impact_probability(data, feature, label=\"stroke\"):\n","    feature_impact = df.groupby(feature)[label].agg([\"sum\", \"size\"])\n","    feature_impact_probability = feature_impact[\"sum\"] / feature_impact[\"size\"] * 100\n","    return feature_impact_probability\n"]},{"cell_type":"markdown","metadata":{"id":"8b6aCbDOn4ai"},"source":["#### ever_married vs stroke"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7O9qsuJmn4ai"},"outputs":[],"source":["## Display bivariate plot for \"ever_married\" field\n","## Hint: use previous function `bivariate_plot()`\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vAUnPWeMn4aj"},"outputs":[],"source":["## Calculate the impact of \"ever_married\" on our target variable\n","## Hint: use previous function `calculate_feature_impact_probability()`\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"GnRE3Eym0kTc"},"source":["<li>List your observations here</li>"]},{"cell_type":"markdown","metadata":{"id":"kKjFCcCRn4aj"},"source":["#### Residence_type vs stroke"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mz8dQ-knn4aj"},"outputs":[],"source":["## Display bivariate plot for \"Residence_type\" field\n","## Hint: use previous function `bivariate_plot()`\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SloolUKfn4aj"},"outputs":[],"source":["## Calculate the impact of \"Residence_type\" on our target variable\n","## Hint: use previous function `calculate_feature_impact_probability()`\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"-IKfB34kJ_TY"},"source":["<li>List your observations here</li>"]},{"cell_type":"markdown","metadata":{"id":"Depo7EBOn4ak"},"source":["#### work_type vs stroke"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGoYoNGMn4ak"},"outputs":[],"source":["## Display bivariate plot for \"work_type\" field\n","## Hint: use previous function `bivariate_plot()`\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6R0Pg0PCn4ak"},"outputs":[],"source":["## Calculate the impact of \"work_type\" on our target variable\n","## Hint: use previous function `calculate_feature_impact_probability()`\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"fsGzLx6wMNjG"},"source":["<li>List your observations here</li>"]},{"cell_type":"markdown","metadata":{"id":"WbKOom5rn4ak"},"source":["#### smoking_status vs stroke"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oIJW1BK8n4ak"},"outputs":[],"source":["## Display bivariate plot for \"smoking_status\" field\n","## Hint: use previous function `bivariate_plot()`\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9vFcVqIn4al"},"outputs":[],"source":["## Calculate the impact of \"smoking_status\" on our target variable\n","## Hint: use previous function `calculate_feature_impact_probability()`\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"Bph_68pHNpvL"},"source":["<li>List your observations here</li>"]},{"cell_type":"markdown","metadata":{"id":"O0DTZ2Kzn4al"},"source":["#### hypertension vs stroke"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ND_soNDRn4al"},"outputs":[],"source":["## Display bivariate plot for \"hypertension\" field\n","## Hint: use previous function `bivariate_plot()`\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h01xeTKln4al"},"outputs":[],"source":["## Calculate the impact of \"hypertension\" on our target variable\n","## Hint: use previous function `calculate_feature_impact_probability()`\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"G64Zit02iAp3"},"source":["<li>List your observations here</li>"]},{"cell_type":"markdown","metadata":{"id":"gTGcHWLWn4am"},"source":["#### heart_disease vs stroke"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgdHkwGCn4am"},"outputs":[],"source":["## Display bivariate plot for \"heart_disease\" field\n","## Hint: use previous function `bivariate_plot()`\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdW_6g78n4am"},"outputs":[],"source":["## Calculate the impact of \"heart_disease\" on our target variable\n","## Hint: use previous function `calculate_feature_impact_probability()`\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"Hq1T1qaMyQJV"},"source":["<li>List your observations here</li>"]},{"cell_type":"markdown","metadata":{"id":"aeaN12_6n4am"},"source":["#### Bonus Content (Optional)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Q2h15-7n4am"},"outputs":[],"source":["@interact(predictor=categorical_features)\n","def interactive_bivariate_barplot(predictor):\n","    bivariate_barplot(df, predictor)"]},{"cell_type":"markdown","metadata":{"id":"XQyJK-oRn4am"},"source":["### Bivariate Analysis: Correlation Matrix and Scatter Plots"]},{"cell_type":"markdown","metadata":{"id":"sdLqZCXLn4an"},"source":["**[Term] Correlation Matrix**\n","\n","A correlation matrix is simply a table which displays the correlation coefficients for different variables. The matrix depicts the correlation between all the possible pairs of values in a table. It is a powerful tool to summarize a large dataset and to identify and visualize patterns in the given data."]},{"cell_type":"markdown","metadata":{"id":"GHm2r8ctn4an"},"source":["The relationship between numeric variables can be further analyzed using a correlation matrix and scatter plots as well."]},{"cell_type":"markdown","metadata":{"id":"TgYrFIwtn4an"},"source":["We can use the pandas method `corr()` to generate the correlation matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qeQNJJ4Jn4an"},"outputs":[],"source":["df[numerical_features].corr()"]},{"cell_type":"markdown","metadata":{"id":"cLB08HENaKUH"},"source":["<li>List your observations here</li>"]},{"cell_type":"markdown","metadata":{"id":"jvD3oXCcn4an"},"source":["Let's see the correlation of numerical variables with the target variable"]},{"cell_type":"markdown","metadata":{"id":"yzVEg0IGn4an"},"source":["**Q:** What happens when you add a `list` datatype to another `list` datatype?\n","```python\n","[1, 2, 3] + [0]\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFKtyFyUn4an"},"outputs":[],"source":["df[numerical_features + [target_variable]].corr()"]},{"cell_type":"markdown","metadata":{"id":"5WHdGwaUn4ao"},"source":["<li>List your observations here</li>"]},{"cell_type":"markdown","metadata":{"id":"3klcwFhCn4ao"},"source":["Let's finally see the scatter plots by defining a reusable function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ql-MXTGbn4ao"},"outputs":[],"source":["def scatter_plot(data, x, y, label=\"stroke\"):\n","    data.sort_values(label).plot.scatter(\n","        x=x,\n","        y=y,\n","        c=label,\n","        colormap=sns.color_palette(\"flare\", as_cmap=True),\n","        figsize=(10, 10),\n","    )"]},{"cell_type":"markdown","metadata":{"id":"XNbBHEXIn4ao"},"source":["#### age vs bmi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OByNbVzOn4ao"},"outputs":[],"source":["scatter_plot(data=df, x=\"age\", y=\"bmi\")"]},{"cell_type":"markdown","metadata":{"id":"oh2re_pJXIDq"},"source":["<li>List your observations here</li>"]},{"cell_type":"markdown","metadata":{"id":"vzjPEloAn4ao"},"source":["#### age vs avg_glucose_level"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"guPbygisn4ao"},"outputs":[],"source":["## display the scatter plot of \"age\" vs \"avg_glucose_level\" on our target variable\n","## Hint: use previous function `scatter_plot()`\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"P2DJzNwPYJuG"},"source":["<li>List your observations here</li>"]},{"cell_type":"markdown","metadata":{"id":"LU7jX0EYn4ap"},"source":["#### bmi vs avg_glucose_level"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MoahvFQ0n4ap"},"outputs":[],"source":["## display the scatter plot of \"avg_glucose_level\" vs \"bmi\" on our target variable\n","## Hint: use previous function `scatter_plot()`\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"I6Y2dkgpn4ap"},"source":["<li>List your observations here</li>"]},{"cell_type":"markdown","metadata":{"id":"fzEZW63Hn4ap"},"source":["# Data Preparation and Feature Engineering"]},{"cell_type":"markdown","metadata":{"id":"z1PTzl7Un4ap"},"source":["### Categorical Encoding"]},{"cell_type":"markdown","metadata":{"id":"6uIPfoq1n4ap"},"source":["**[Term] Categorical Encoding**\n","\n","Encoding categorical data is a process of converting categorical data into integer format so that the data with converted categorical values can be provided to the different models."]},{"cell_type":"markdown","metadata":{"id":"pmh9v7gxn4aq"},"source":["There are two most widely used encoding techniques:\n","1. Label Encoding\n","2. One-Hot Encoding"]},{"cell_type":"markdown","metadata":{"id":"zx0sxKszn4aq"},"source":["**[Term] Label Encoding**\n","\n","Label Encoding is a popular encoding technique for handling categorical variables. In this technique, each label is assigned a unique integer based on alphabetical ordering."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9NylH2rn4aq"},"outputs":[],"source":["df.gender.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3JWerxCHn4aq"},"outputs":[],"source":["df.gender.astype(\"category\").cat.codes.value_counts()"]},{"cell_type":"markdown","source":[""],"metadata":{"id":"GVhkD6akf0H_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6-BQwrfn4aq"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"malj6NUSn4aq"},"outputs":[],"source":["def fit_label_encoder(feature):\n","    le = LabelEncoder()\n","    le.fit(feature)\n","    return le"]},{"cell_type":"markdown","metadata":{"id":"o0gnNJShn4aq"},"source":["**[Term] One-Hot Encoding**\n","\n","One-Hot Encoding is another popular technique for treating categorical variables. It simply creates additional features based on the number of unique values in the categorical feature. Every unique value in the category will be added as a feature. **One-Hot Encoding is the process of creating dummy variables.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"74qpzVX2n4ar"},"outputs":[],"source":["pd.get_dummies(df.work_type)"]},{"cell_type":"markdown","metadata":{"id":"wJWJwgnSn4ar"},"source":["### Choosing the correct encoder\n","\n","It's good to use a label encoder when the variables have a numeric association with each other, for example, grades and ranks.\n","\n","We can use one-hot encoding for unassociated categorical variables.\n","\n","However, One-Hot Encoding results in a *Dummy Variable Trap* as the outcome of one variable can easily be predicted with the help of the remaining variables. Dummy Variable Trap is a scenario in which variables are highly correlated to each other. In order to overcome this problem, one of the dummy variables has to be dropped.\n","\n","So, a One-Hot Encoding system of two variables is basically just a Label Encoding system."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jQzHaxn1n4ar"},"outputs":[],"source":["def one_hot_encoder(data, feature_name, column_to_drop=None):\n","    modified_data = data.copy()\n","    feature = modified_data[feature_name]\n","\n","    if column_to_drop is None or column_to_drop not in feature.unique():\n","        # Drop the column with least number of data points\n","        column_to_drop = feature.value_counts().index[-1]\n","\n","    dummies = pd.get_dummies(feature)\n","    modified_data[dummies.columns] = dummies\n","\n","    return modified_data.drop([column_to_drop, feature_name], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_gZ10IgDn4ar"},"outputs":[],"source":["feature_columns = categorical_features + numerical_features\n","features = df[feature_columns].copy()\n","\n","gender_encoder = fit_label_encoder(feature=features.gender)\n","features[\"gender\"] = gender_encoder.transform(features[\"gender\"])\n","\n","marital_status_encoder = fit_label_encoder(feature=features.ever_married)\n","features[\"ever_married\"] = marital_status_encoder.transform(features[\"ever_married\"])\n","\n","residence_encoder = fit_label_encoder(feature=features.Residence_type)\n","features[\"Residence_type\"] = residence_encoder.transform(features[\"Residence_type\"])\n","\n","features = one_hot_encoder(features, \"work_type\", column_to_drop=\"Never_worked\")\n","features = one_hot_encoder(features, \"smoking_status\", column_to_drop=\"Unknown\")\n","features.head()"]},{"cell_type":"markdown","metadata":{"id":"oiiMdfAEn4ar"},"source":["### Data Imputation"]},{"cell_type":"markdown","metadata":{"id":"pFtt18Lrn4ar"},"source":["**[Term] Data Imputation**\n","\n","In statistics, imputation is the process of replacing missing data with substituted values."]},{"cell_type":"markdown","metadata":{"id":"iTFwHLxwdwhH"},"source":["There are many ways to handle missing data\n","\n","1. Drop data\n","\n","    a. Drop the entire row\n","      - The whole row can be dropped\n","        - if the target variable that we are trying to predict is missing\n","        - if majority of data item in a row is missing\n","\n","    b. Drop the whole column\n","      - Whole columns can be dropped if most entries in the column are empty\n","\n","2. Replace data\n","\n","    a. Replace it by mean\n","      - Replace the missing value in a column with the mean\n","      - Only works on column level\n","      - Mean imputation can't be used on categorical features i.e. only works with continuous feautres\n","      - Problem may occur when there is outliers in the dataset as outlier value may influence more in computing the mean\n","\n","    b. Replace it by median\n","      - Replace the missing value in a column with the median\n","      - Only works on column level\n","      - Median imputation can't be use on categorical features i.e. only works with continuous features\n","      - Better imputation strategy compared to the mean in the presence of outliers in the dataset\n","\n","    c. Replace it by mode (frequency)\n","      - Replace the missing value in a column with the mode (i.e. maximum occuring item in a column)\n","      - Only works on column level\n","      - Mode imputation is suitable for categorical features and is not suitable for continuous or discrete numerical features\n","\n","    d. Replace it based on other functions"]},{"cell_type":"markdown","metadata":{"id":"YgmhggZfn4as"},"source":["\n","In our case, the *bmi* column contains 201 missing data i.e. around 4%, so it is not suitable to drop the whole *bmi* column. Instead we need to impute the missing data with the best replacement strategy.\n","\n","We will use `KNNImputer` from Sklearn to imput a missing values in bmi column. Under the hood it's also a replace by mean strategy.\n","\n","#### KNNImputer\n","  - Here, each sample's missing values are imputed using the mean value from `n_neighbours` nearest neighbors found in the training set. \n","  - Two samples are close (neighbors) if the features that neither is missing are close.\n","  - Closeness is computed using euclidean distance.\n","\n","Inorder to use KNNImputer to fill missing values, all of our dataframe values should be in numerical form, since we will compute euclidean distance to find neighbors to missing data point. So, we will be using the encoded features instead of the original data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DHJfdKgKe00Y"},"outputs":[],"source":["from sklearn.impute import KNNImputer\n","\n","\n","imputter = KNNImputer(n_neighbors=30)\n","imputted_data = imputter.fit_transform(features)\n","imputted_data"]},{"cell_type":"markdown","metadata":{"id":"CPrYmwuzn4at"},"source":["This is just a numpy array so we need to convert it back to a DataFrame"]},{"cell_type":"markdown","metadata":{"id":"3LvvpWqUn4at"},"source":["**Q:** How do you display the summary information of a DataFrame?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Le9FuTMn4at"},"outputs":[],"source":["features = pd.DataFrame(data=imputted_data, columns=features.columns)\n","features.info()"]},{"cell_type":"markdown","metadata":{"id":"gDYQ1HOhn4at"},"source":["Put the imputed data into the original dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mcWrJihxn4at"},"outputs":[],"source":["df[\"bmi\"] = features[\"bmi\"]"]},{"cell_type":"markdown","metadata":{"id":"Mtwy-gp7n4ax"},"source":["### Outlier Detection"]},{"cell_type":"markdown","metadata":{"id":"zu7H3E_On4ax"},"source":["Boxplots are the best way to detect outliers in a numerical variable"]},{"cell_type":"markdown","metadata":{"id":"aArLLbs-n4ax"},"source":["#### age"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s6vaQ8zbn4ax"},"outputs":[],"source":["plt.figure(figsize=(10, 10))\n","sns.boxplot(data=features.age)"]},{"cell_type":"markdown","metadata":{"id":"9PYqymH7kqxU"},"source":["<li>There is no presence of outlier in Age column.</li>"]},{"cell_type":"markdown","metadata":{"id":"1P_JqdBBn4ay"},"source":["**[Term] Interquartile Range (IQR)**\n","\n","In descriptive statistics, the interquartile range is a measure of statistical dispersion, which is the spread of the data. It is defined as the difference between the 75th and 25th percentiles of the data."]},{"cell_type":"markdown","metadata":{"id":"RZCT6pKXn4ay"},"source":["**[Term] IQR Filter**\n","\n","This technique uses the IQR to remove or impute outliers. The rule of thumb is that anything not in the range of (Q1 - 1.5 * IQR) and (Q3 + 1.5 * IQR) is an outlier, and can be removed or imputed."]},{"cell_type":"markdown","metadata":{"id":"nUp8U2Wyn4ay"},"source":["Let's write a function to get the interquartile range"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_A10F_ycn4ay"},"outputs":[],"source":["def get_iqr(feature):\n","    quantile1, quantile3 = np.percentile(feature, [25, 75])\n","    iqr_value = quantile3 - quantile1\n","    lower_bound = quantile1 - 1.5 * iqr_value\n","    upper_bound = quantile3 + 1.5 * iqr_value\n","    return lower_bound, upper_bound"]},{"cell_type":"markdown","metadata":{"id":"A4RcaQhNn4ay"},"source":["#### bmi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xqF0NGSsn4ay"},"outputs":[],"source":["plt.figure(figsize=(10, 10))\n","sns.boxplot(data=features.bmi)"]},{"cell_type":"markdown","metadata":{"id":"FCminYDglZB_"},"source":["<li>From the given boxplot, we can see that there are many values that is beyond the 3 standard deviation.</li>\n","\n","<li>Since, the bmi column is not gaussian distributed, we have to use inter quantile range for detecting the outliers</li>"]},{"cell_type":"markdown","metadata":{"id":"YPcQGhPKqJ2C"},"source":["<li>It is not always that outliers are not important for predictive modelling.</li>\n","\n","<li>In predicting minority class in a class imbalance problem, outliers can play an important role for our model.</li>\n","\n","<li>Since, we also have class imbalance problem in our dataset, we donot remove the outliers.</li>\n","\n","<li>Instead of removing it, we have to deal with it smartly.</li>\n","\n","<li>So we are replacing the extreme outlier values with the mean of values that are deviated from the inter quantile range.</li>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"et_DOEbvn4az"},"outputs":[],"source":["lower, upper = get_iqr(features.bmi)\n","outlier_condition = (features.bmi < lower) | (features.bmi > upper)\n","features[outlier_condition] = features[outlier_condition].mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hWSoUoS2n4az"},"outputs":[],"source":["plt.figure(figsize=(10, 10))\n","sns.boxplot(data=features.bmi)"]},{"cell_type":"markdown","metadata":{"id":"YPqA5lvPn4az"},"source":["#### avg_glucose_level"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SEyHZJacn4az"},"outputs":[],"source":["plt.figure(figsize=(10, 10))\n","sns.boxplot(data=features.avg_glucose_level)"]},{"cell_type":"markdown","metadata":{"id":"Xo_Rj_2RsJRF"},"source":["<li>We can see a similar outlier trend in the <i>avg_glucose_level</i></li>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5WW_9z3rn4az"},"outputs":[],"source":["lower, upper = get_iqr(features.avg_glucose_level)\n","outlier_condition = (features.avg_glucose_level < lower) | (features.avg_glucose_level > upper)\n","features[outlier_condition] = features[outlier_condition].mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PHkwoVSkn4az"},"outputs":[],"source":["plt.figure(figsize=(10, 10))\n","sns.boxplot(data=features.avg_glucose_level)"]},{"cell_type":"markdown","metadata":{"id":"jmV8SSmzn4a0"},"source":["### Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"i1bFrlacn4a0"},"source":["Let us now prepare training and test data.\n","\n","Training data and test data sets are two different but important parts in machine learning. While training data is necessary to teach an ML algorithm, testing data, as the name suggests, helps you to validate the progress of the algorithm's training and adjust or optimize it for improved results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dVkyRD8wn4a0"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7w9fZtgAn4a0"},"outputs":[],"source":["X = df[categorical_features + numerical_features].copy()\n","y = df[target_variable].copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEEj7u_1s8nz"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(\n","    X,\n","    y,\n","    # To distribute each class in the train and test set evenly\n","    stratify=y,\n","    # Take a test size of 20%\n","    test_size=0.2,\n","    # Set a seed so that we can reproduce the result\n","    random_state=42,\n",")\n","X_train.reset_index(drop=True,inplace=True)\n","X_test.reset_index(drop=True,inplace=True)\n","y_train.reset_index(drop=True,inplace=True)\n","y_test.reset_index(drop=True,inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"EAp6wPGen4a0"},"source":["Let's check whether stratified split worked or not. To do that, we can check the percentage of data of each class in the train and test labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xFrtDp1pn4a0"},"outputs":[],"source":["y_train_counts = y_train.value_counts()\n","y_train_counts / y_train_counts.sum() * 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g9_kDgJEn4a0"},"outputs":[],"source":["y_test_counts = y_test.value_counts()\n","y_test_counts / y_test_counts.sum() * 100"]},{"cell_type":"markdown","metadata":{"id":"-Z7wyEOrn4a1"},"source":["We can see that we have similar amount of data for each class in the train and test set"]},{"cell_type":"markdown","metadata":{"id":"-tj36b1Fn4a1"},"source":["Now, let's generate the features for both train and test datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFGquYSbtYPA"},"outputs":[],"source":["gender_encoder = fit_label_encoder(X_train[\"gender\"])\n","X_train[\"gender\"] = gender_encoder.transform(X_train[\"gender\"])\n","X_test[\"gender\"] = gender_encoder.transform(X_test[\"gender\"])\n","\n","marital_status_encoder = fit_label_encoder(X_train[\"ever_married\"])\n","X_train[\"ever_married\"] = marital_status_encoder.transform(X_train[\"ever_married\"])\n","X_test[\"ever_married\"] = marital_status_encoder.transform(X_test[\"ever_married\"])\n","\n","\n","residence_location_encoder = fit_label_encoder(X_train[\"Residence_type\"])\n","X_train[\"Residence_type\"] = residence_location_encoder.transform(X_train[\"Residence_type\"])\n","X_test[\"Residence_type\"] = residence_location_encoder.transform(X_test[\"Residence_type\"])\n","\n","X_train = one_hot_encoder(X_train, \"work_type\", column_to_drop=\"Never_worked\")\n","X_test = one_hot_encoder(X_test, \"work_type\", column_to_drop=\"Never_worked\")\n","\n","X_train = one_hot_encoder(X_train, \"smoking_status\", column_to_drop=\"Unknown\")\n","X_test = one_hot_encoder(X_test, \"smoking_status\", column_to_drop=\"Unknown\")\n"]},{"cell_type":"markdown","metadata":{"id":"vzBHnl3mn4a1"},"source":["### Feature Normalization"]},{"cell_type":"markdown","metadata":{"id":"D5g1E6oYn4a1"},"source":["Given different input features with varying scales, feature normalization and standardization are used to guarantee that some machine learning models can work and also help to improve the model's training speed and performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ml9Tkauwn4a1"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pw6h7Tb3w5Sx"},"outputs":[],"source":["scale = StandardScaler()\n","\n","X_train[numerical_features] = scale.fit_transform(X_train[numerical_features])\n","X_test[numerical_features] = scale.transform(X_test[numerical_features])"]},{"cell_type":"markdown","metadata":{"id":"03cmuAatn4a1"},"source":["# Model Training"]},{"cell_type":"markdown","metadata":{"id":"spjl-beQkycw"},"source":["## **Logistic Regression**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HRccRYz8lCQP"},"source":["*TL:DR: Logistic regression is a classification machine learning algorithm(supervised) used in binary or multi-class classification. It tries to estimate the class of input features using S-shaped sigmoid function/curve which indicates the likelihood of something. The value from sigmoid function always ranges [0,1] (probablistic value)*\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rTdfEY8TJKCT"},"source":["* Logistic regression is one of the most popular classification Machine Learning algorithms, which comes under the Supervised Learning technique. It is used for predicting the categorical dependent variable using a given set of independent variables.\n","\n","* Logistic regression predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.\n","\n","\n","\n","* In Logistic regression, instead of fitting a regression line, we fit an \"S\" shaped logistic function, which predicts two maximum values (0 or 1).\n","\n","<img src= \"https://static.javatpoint.com/tutorial/machine-learning/images/logistic-regression-in-machine-learning.png\" />\n","\n","\n","The curve from the logistic function indicates the likelihood of something such as whether the patient will have heart stroke or not.\n","\n","\n","**Note: Logistic Regression is much similar to the Linear Regression except that how they are used. Linear Regression is used for solving Regression problems, whereas Logistic regression is used for solving the classification problems.**\n"]},{"cell_type":"markdown","metadata":{"id":"HCpxrMOiGJYS"},"source":["### **Logistic Function (Sigmoid Function):**\n","\n","$$h(x) = g(z) = \\frac 1 {1 + e^{-z}} = \\frac 1 {1 + e^{-(\\beta_0 + \\beta_1x)}}$$ \n","\n","\n","\n","* The sigmoid function is a mathematical function used to map the predicted values to probabilities. It maps any real value into another value within a range of 0 and 1. The value of the logistic regression must be between 0 and 1, which cannot go beyond this limit, so it forms a curve like the \"S\" form. The S-form curve is called the Sigmoid function or the logistic function.\n","\n","\n","* In logistic regression, we use the concept of the threshold value, which defines the probability of either 0 or 1. Such as values above the threshold value tends to 1, and a value below the threshold values tends to 0.\n"]},{"cell_type":"markdown","metadata":{"id":"ikE3KOBzKKcq"},"source":["### **How to use Logistic Regression**\n","\n","Use Scikit-learn machine learning library to train the Logistic regression model.\n","\n","```python\n","from sklearn.linear_model import LogisticRegression\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kKOSXe_hWXvk"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","clf = LogisticRegression(random_state=0)"]},{"cell_type":"markdown","metadata":{"id":"vnHvXn8fOlLh"},"source":["### What is random_state?\n","\n","**random_state** is used to set the seed for the random generator so that we can ensure that the results that we get can be reproduced."]},{"cell_type":"markdown","metadata":{"id":"ql85F723PKiA"},"source":["## Classical Programming Vs Machine Learning\n","<img src=\"https://miro.medium.com/max/799/1*t6Myx_4eEwaWP9Vms_kYfg.png\"/>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oT2rHDDnPWRI"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","clf = LogisticRegression(random_state=0)\n","clf.fit(X_train, y_train) # pass training data and training answer to model"]},{"cell_type":"markdown","metadata":{"id":"tXjVNK1pPzze"},"source":["# **Model Evaluation**\n","\n","We developed the Machine learning model. Is it an accurate model?\n","\n","The evaluation metric depends on the machine learning algorithm. Since we have used Logistic regression. The evaluation metric would be to measure the accuracy of classification. Other metrics also exist like f1-scores, ROC, AUC, etc. for classification problems."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQ-JJmTZO4YM"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, f1_score\n","# from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_score, f1_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nt-AU5F7RH_v"},"outputs":[],"source":["# predict for the test data\n","y_pred = clf.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9CWOTOZPRQju"},"outputs":[],"source":["print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.3f}%\")"]},{"cell_type":"code","source":["print(f\"Accuracy: {f1_score(y_test, y_pred) * 100:.3f}%\")"],"metadata":{"id":"MXBRjXOUI0II"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ee7JyF-XYICE"},"source":["The accuracy looks good. Lets see the matrix of classification/ Confusion matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FI_m5uwYP9M"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, display_labels=clf.classes_, cmap=plt.cm.Blues)"]},{"cell_type":"markdown","metadata":{"id":"BuU0KV87TDb8"},"source":["### Sometimes accuracy is not a good metric\n","\n","Let's see the count distribution and percentage of the target variable to see how many data points are there for different class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SjaXi30YVQHK"},"outputs":[],"source":["y_train.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZOXKdBGTM-X"},"outputs":[],"source":["y_train.value_counts(normalize=True)"]},{"cell_type":"markdown","metadata":{"id":"jsonpAxLUs3w"},"source":["Class 1 has low no. of data points. \n","\n","If we just write \n","\n","```python\n","print(\"0\")\n","```\n","for every Input we will get 95% accuracy.Therefore, for an imbalanced dataset, accuracy is not a good metric.\n"]},{"cell_type":"markdown","metadata":{"id":"TmgrrD7xbxOE"},"source":["## Anatomy of confusion matrix\n","\n","\n","<img src= \"https://miro.medium.com/max/1400/1*LQ1YMKBlbDhH9K6Ujz8QTw.jpeg\" width=\"600\" height=\"300\"/>\n","\n","\n","In confusion matrix, the rows corresponds to what machine learning model predicts and the columns correspond to actual values. Now lets understand the terms TP, FP,TN,FN.\n","\n","\n","**True Positive:**\n","\n","Interpretation: You predicted that a person is diabetic and he/she actually is.\n","\n","**True Negative:**\n","\n","Interpretation: You predicted that a person is not diabetic and he/she actually is not.\n","\n","**False Positive: (Type 1 Error)**\n","\n","Interpretation: You predicted that a person is diabetic and he/she actually is not.\n","\n","**False Negative: (Type 2 Error):**\n","\n","Interpretation: You predicted that a person is not diabetic and he/she actually is."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STRMUYKlVy6E"},"outputs":[],"source":["ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, display_labels=[\"Not Stroke\", \"Stroke\"], cmap=plt.cm.Blues)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"R3kYbtw3eCOz"},"source":["Other evaluation metrics:\n","\n","\n","<img src=\"https://miro.medium.com/max/1068/1*EXa-_699fntpUoRjZeqAFQ.jpeg\"/>"]},{"cell_type":"markdown","metadata":{"id":"VgiCNRhReimv"},"source":["For simplicity check F1-score of the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCC4P2Z4eVXD"},"outputs":[],"source":["f1_score(y_test, y_pred) *100"]},{"cell_type":"markdown","metadata":{"id":"G2_6lDpNe8oV"},"source":["Model is not leanring anything."]},{"cell_type":"markdown","metadata":{"id":"8tzRxeZLfFX7"},"source":["### Why is the model not learning anything? \n","\n","Since, the target class \"1\" is only 5% of the whole dataset. So, we are facing this issue.\n","Let's do oversampling of underrepresented class on our training set to handle class imbalance problem. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oBNoJ9j1e-S1"},"outputs":[],"source":["from imblearn.over_sampling import SMOTE\n","smote = SMOTE()\n","X_train_upsampled, y_train_upsampled = smote.fit_resample(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z9pPdMjon4a5"},"outputs":[],"source":["X_train.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UJ819Azyn4a5"},"outputs":[],"source":["X_train_upsampled.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BflcZe6vfmpK"},"outputs":[],"source":["y_train.value_counts(normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k2hf3Y0DfW8O"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import ConfusionMatrixDisplay\n","clf = LogisticRegression(random_state=0)\n","clf.fit(X_train_upsampled, y_train_upsampled) # pass training data and training answer to model\n","ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, display_labels=clf.classes_, cmap=plt.cm.Blues)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"zSuZsAxcf-7X"},"source":["It seems like model is learning. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sukGngxCf972"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","y_pred = clf.predict(X_test)\n","print(\n","    \" Accuracy: {:.3f}%      F1-Score: {:.5f}\"\\\n","    .format(accuracy_score(y_test, y_pred) * 100, f1_score(y_test, y_pred) * 100)\n",")"]},{"cell_type":"code","source":["#@title Optional [Other Smote Technique]\n","from imblearn.over_sampling import BorderlineSMOTE\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","\n","bsmote = BorderlineSMOTE(random_state = 101, kind = 'borderline-1')\n","X_oversample_borderline, y_oversample_borderline = bsmote.fit_resample(X_train, y_train)\n","\n","\n","classifier_border = LogisticRegression(random_state = 40)\n","classifier_border.fit(X_oversample_borderline, y_oversample_borderline)\n","\n","\n","y_pred = classifier_border.predict(X_test)\n","print(\n","    \" Accuracy: {:.3f}%      F1-Score: {:.5f}\"\\\n","    .format(accuracy_score(y_test, y_pred) * 100, f1_score(y_test, y_pred) * 100)\n",")"],"metadata":{"cellView":"form","id":"kI76-HD6JSxk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PagqWQprn4a6"},"source":["#### [Term] Hyperparameter Tuning\n","\n"]},{"cell_type":"markdown","metadata":{"id":"G_yoVq9In4a6"},"source":["[Example](https://www.kaggle.com/code/funxexcel/p2-logistic-regression-hyperparameter-tuning)"]},{"cell_type":"markdown","source":["# What's Next?"],"metadata":{"id":"M72b2nlqiGOj"}},{"cell_type":"markdown","source":["<img src = \"https://i.pinimg.com/originals/02/81/f9/0281f9d2d8b8c9f2801843a1f7445977.png\"/>"],"metadata":{"id":"nay1-wt9iTTP"}},{"cell_type":"markdown","source":["# What are some other popular classification algorithms?\n","\n","<img src = \"https://static.javatpoint.com/tutorial/machine-learning/images/classification-algorithm-in-machine-learning.png\"/>\n","\n","* Logistic Regression\n","* K-Nearest Neighbors\n","* Decision Tree\n","* Support Vector Machines/SVC\n","* Naive Bayes"],"metadata":{"id":"fN3-u6pRiJSb"}},{"cell_type":"markdown","source":["# Resources \n","\n","* Implementation of Machine Learning Algorithm from Scratch: [Link](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch)"],"metadata":{"id":"J1pv9eEKg33h"}},{"cell_type":"code","source":[""],"metadata":{"id":"6vyoXbB-gWoX"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.8 ('.venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"1db9252b0c60d0e6349e5c57bf19a68c333862a9d2673e2b28969c78c4e62b82"}},"colab":{"name":"ai-workshop-student-version.ipynb","provenance":[],"collapsed_sections":["hMhW5LSS4pjp","aM4TAmWnn4aL","HCpxrMOiGJYS","vnHvXn8fOlLh"]}},"nbformat":4,"nbformat_minor":0}